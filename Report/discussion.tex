\section{Discussion}

\paragraph{Feature Selection}
Reasons for feature selection are to reduce overfitting and eliminate possible truly redundant features -- moreover, reducing the number of features can simplify the model and make it easier to interpret. In this work we focused mainly on potential performance benefits, but the results from table \ref{tab:feature_relevance} can also be useful for identifying feature subsets with potential to outperform full set. For example, taking only features with non-zero Infogain is one possible approach.
% * <philip@thruesen.dk> 2016-06-01T19:00:00.702Z:
%
% outcommented a paragraph. I dont believe using a subset of features will ever perform better than the full set unless there's overfitting which I think is very unlikely for our model.
%
% ^ <jarekcechak@gmail.com> 2016-06-02T09:10:39.850Z:
%
% This is what Nattiya told us that feature selection is usually used for
%
% ^ <jarekcechak@gmail.com> 2016-06-02T09:10:44.022Z.
% * <philip@thruesen.dk> 2016-06-01T18:56:01.192Z:
%
% > truly redundant
%
% changed conflicting to truly redundant. Change back if you disagree
%
% ^ <roelcastanomoreno@gmail.com> 2016-06-02T09:12:22.872Z.

All three methods used to evaluate the features are only heuristics and don't completely reflect true value of features. They are mainly focused on isolated importance of single feature, there might be however, complex relations between some features that will not be identified this way.

\paragraph{Applicability of Model. }
It is interesting to discuss the realistic applicability of the models generated by this research. We realize that having a fixed threshold for the number of links that may appear in an article is a na\"{\i}ve solution to the problem of overlinking. The correct number of links that may appear in an article should rather be determined by some function that considers the length of the article. Computing this is outside the scope of this work and any limits are best to be defined by Wikipedia policy makers. Depending on the use case of the suggested model, the degree to which the feature selection is needed can be reconsidered, i.e. how many resources to dedicate to the training and use of such a model.
%With that in mind and some possible improvements (Choosing a correct threshold for the number or percentage of links), 
Summarizing these thoughts, we think the model applies well to Wikipedia articles and possibly other wiki-based sites. There's always a risk that some articles `lose' a small percentage of useful links if the model was used with full automation but the benefits of improving readability through Wikipedia might out-weight the drawbacks. It may also simply be used as a tool for editors, alerting them when they are about to add a link that might confuse future readers more than it would help them.

\paragraph{Future Work}
Naturally, it is possible to expand the work done in this article. Many aspects of the research were limited by significant restraints on processing power and time availability. If more resources were available the project could be extended and up-scaled in multiple obvious ways.%, but it can be easily extended by building upon the different components.

For the future, one might choose to increase amount of articles in the dataset and change the strategy with which they are selected.  For this article, we chose the 1000 articles having the most outgoing clicks. In this way we were sure to have only articles where a ranking of referred articles was possible. Had we e.g. included articles in the prominent set without outgoing clicks we would have to rank a set of referred articles all assigned to the same label (of zero). %Another strategy to select prominent articles might be to choose one or multiple clusters of articles to generate the model or a random sample.

There's much work that could be done for the set of features. E.g. the features describing link position could be supplemented with the link position based on the visual representation after HTML had been rendered. Moreover, some features describing explicit categories could be added and the 'generality' of an article, which represents how specific or general the topic discussed is (to the general public). We have skipped construction of some potential features like the ones mentioned above due to prioritization considering our limited time.

Finally, another possibility is to evaluate other implementations of Learning to Rank algorithms or modify the existing ones. This was also outside of the scope of the project. 