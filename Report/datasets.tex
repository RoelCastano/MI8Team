\section{Data Sets}

There are multiple data sets and sources available which facilitate the interaction with Wikipedia, and due to the massive amount of data managed by wikipedia, it is important to have an organised and manageable representation of this data in our own server for processing. This section describes the data sources used during the research of this article.

\subsubsection{Wikipedia Dumps}
The most significant data source is the English Wikipedia database dump, which is released at least once a month, and contains a complete copy of the text and metadata of current revisions of all articles in XML format. This dump facilitates the extraction of mu Wikipedia also releases the page views and page counts for all articles.

\subsubsection{DBpedia}
Another source that proved to be useful for the formation of our data sets was DBpedia, which provides a wide range of datasets for every wikipedia language on more than 10,000 articles. Given that we used a small sample of the top articles in the english version of Wikipedia, it was possible to make use of this data. Some examples of the data sets include page ids, page length, categories, links, etc.

\subsubsection{Wikipedia Clickstream}
The Wikipedia Clickstream \cite{wulczyn} project contains data sets of $(referer, resource)$ pairs of articles describing user navigation and raw counts on the volume of traffic through the article pairs. This pairs are extracted from the request logs of Wikipedia, in which the referer is an HTTP header field that identifies the webpage from which the resource was requested. Typical referral sites like Google or Facebook are also included and crawler-traffic has been attempted filtered from the raw data.