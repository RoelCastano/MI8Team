\section{Data Sets}

There are multiple data sets and sources available which facilitate the interaction with Wikipedia, and due to the massive amount of data managed by wikipedia, it is important to have an organised and manageable representation of this data in our own server for processing. The most significant data source is the English Wikipedia database dump, which is released once a month, and contains the text and metadata of current revisions of all articles as XML files. Most features rely in some way or another in the database dumps since it supplies the structure and content of the articles. Wikipedia also releases the page views and page counts for all articles. \\
Another source that proved to be useful for the formation of our data sets was DBpedia, which provides a wide range of datasets for every wikipedia language on more than 10,000 articles. Given that we used a small sample of the top articles in the english version of Wikipedia, it was possible to make use of this data. Some examples of the data sets include page ids, page length, categories, links, etc. \\
Last but not least, the Wikipedia Clickstream \cite{wulczyn} project gives pairs of articles describing user navigation and includes raw counts on the volume of traffic through the specific article pairs. Typical referral sites like Google or Facebook are also included and crawler-traffic has been attempted filtered from the raw data. As explained later in the article, we build the ground truth for our model primarily using this data set.