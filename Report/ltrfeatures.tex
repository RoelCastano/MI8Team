\section{Features}
This section defines each feature used in the implementation of Learn to Rank algorithm.

\subsection{Link Position}
Our intuition behind the Link Position feature is based on two reasons. The first one is that given the way Wikipedia articles are structured, the most general description of the article is placed in the first few paragraphs before the table of contents. This section of the Wikipedia article is called the lead \cite{lead}. As described in the Wikipedia manual of style, for many people, the lead section is the only section they will read since it summarises the entire article. Later paragraphs only dive deeper into the topics outlined in the description. \\
The second motivation is the way people read web pages. As explained by Jakob Nielsen  \cite{nielsen}, one of the leaders in human-computer interaction, on average, users have time to read at most 28\% of the words of a website. Additionally, most attention is given to the top portion of a page and later sections are merely skimmed through. People looking for different article that is somewhat related to the one currently being read might be interested in more general concepts as they contain the searched term. As explained above, more general terms happen to be heavily abundant in the first portion of Wikipedia article. \\
To be able to measure this feature, we count the number of characters preceding the occurrence of the link. The text of the articles is taken from the wikipedia dumps previously described and links to other wikipedia articles are found using regular expressions. As expected, links to images, external sources, etc. are ignored. Due to the way this feature is extracted, there might be slight discrepancies in feature value and the exact number of characters preceding the links. \\

\subsection{Link Order}
Similar to the previous feature, Link Order is based on the fact that a wikipedia article's initially describe the topic in general terms and the hypothesis?? that the probability of clicking a link is higher the closer it is to the first term. While the link position captures more of a distance between links and their spread, link order is a simplified version of it. It conveys less information, but in a much more straightforward manner. \\
This feature is also extracted from the Wikipedia dumps by counting the number of links in the article. This feature captures the position of link relative to all the other links contained in the same article. The value $n$ means that the link is the $n^{th}$ link in the article. \\

\subsection{Community Membership}
This feature captures notion of two article being in the same community. The communities are computed from the graph representation of Wikipedia $G(V,E)$, where $V$ is a set of articles and $E$ is set of links between them. In this scenario community is $(V', E') = G' \subseteq G$ such that $|E'| \ge |\{ \{u,v\} \in E\setminus E'  \quad | \quad u \in V' \vee v \in V' \}|$. Communities are a implicit way of clustering articles on Wikipedia that capture emerging properties of interconnected articles.  \\

This feature looks promising in cases where someone is interested in a specific topic. This could mean, as mentioned earlier, bands of a music genre, states of a country, fields of study in a certain science, or many others. Related articles from the same community might be a good place to look at and will highly likely contain desired article. Furthermore, users reading an article have shown an interest in specific topic and might want to broaden and deepen his or her knowledge of it.

\subsection{Symmetric Linking}
This feature captures the notion of two article being interconnected in both directions. Formally, a link $(A,B)$ between article A and B is symmetric if and only if link $(B,A)$ also exists. Symmetric linking indicates, in some cases, that there exists an important relevance between said articles or highly related topics are being discussed. Examples of this article relationship includes competing presidential candidates, sports team rivals, movies and its actors, etc. As expected, it is common for users to demonstrate interest in these kind of relations between articles. By looking and the article relationships, we found in one sample of the most visited articles that 74.9\% of the links were non-symmetric and 25.1\% were symmetric.

\subsection{HITS and PageRanks}

Although HITS and PageRank differ in content their main goal is the same, give estimate of article significance. In case of HITS, high hub score may indicate more general topics while high authority score would be indication of very focused article discussing particular term in great depth. PageRank is similar to the hub and authority score combined. \\

Similarly to community extraction, HITS and PageRank are computed from graph representation of Wikipedia. The graph is processed in R using package igraph that implements both scores. \\

The intuition behind this feature comes from the search engine domain. In search, these scores helps identify the most relevant results. When applied to articles, the scores will help categorise linked articles for the learning to rank algorithm.

\subsection{Relatedness obtained from links}

This feature measures how two articles are related in terms of linking. Indeed, it compares the set of links (both incoming and outgoing links) of the current article and the target one. The more links they have in common, the more \"\ related\"\ the two articles are. \

In order to quantify the similarity between links sets, we computed three different metrics  : Jaccard coefficient, Dice\'\ s measure and Normalized Google Distance. \
These three metrics are shown in Table ~\ref{fig:formulas}. Given two articles a and b, A and B will respectively stand for the set of articles linking (considering both incoming and outgoing links) to article a and the set of articles linking to b. W represents the number of articles in the all set.

\begin{table}[H]
\caption{Relatedness measure formulas and values range}
\centering
\begin{tabular}{llr}
\toprule
\cmidrule(r){1-2}
Formula & Min. & Max.\\
\midrule
$Dice(a,b)=2\times\frac{|A \cap B|}{|A| + |B|}$ & 0\% & 100\% \\
 & & \\
$Jaccard(a,b)= \frac{|A\cap B|}{|A\cup B|}$ & 0\% & 100\% \\
 & & \\
$NGD(a,b)= \frac{log(max(|A|,|B|))-log(|A\cap B|))}{log(|W|)-log(min(|A|,|B|))}$ & 0 & $\infty $ \\
\bottomrule
\label{fig:formulas}
\end{tabular}
\end{table}

\subsubsection*{Dice's measure}
Dice's measure is the fraction of the links both article have in common to the sum of links they both have. Then it has a very intuitive meaning. The highest Dice's measure is, the more related articles are.\

\subsubsection*{Jaccard coefficient}
Jaccard coefficient is quite similar to Dice's measure. It measures the ratio between the number of links both pages have in common and the total number of links they have.\

\subsubsection*{Normalized Google distance}
This measure was originally based on term occurrences in web pages and is used by Google search engine. As explained by David Milne and Ian H. Witten in their article \textit{"An Effective, Low-Cost Measure of Semantic Relatedness Obtained from Wikipedia Links"}, this measure can also be adapted to measure relatedness between two Wikipedia articles. The smallest the distance is, the more related articles should be.\

\subsubsection*{}
We chosed to integrate this relatedness feature because we made the assumption that a user who’s reading an article about a specific topic might be interested in reading more articles about the same domain. 

As described for the link position feature, links sets are extracted from the raw articles text from Wikipedia dumps.

\subsection{Textual similarity}

This feature measures the textual content similarity between two Wikipedia articles.  \\

The idea behind this feature is that two articles containing approximately the same words are likely to discuss closely related subjects. This is another way to know how the two articles are related on a more semantic level than the relatedness feature we described before.\\

The text of all articles is extracted from wikipedia dumps. We get rid of all tables and “non-text information” (tables,links etc ...) with regular expressions, and we only keep the different paragraphs of each article. Then, for both articles A and B we create a vector that describe the frequencies of the relevant words (TF-IDF vector).\

Then the method consists in comparing the frequencies of relevant words between article A and article B by using the cosine of these two vectors. The values we obtain are spread from 0 to 1 (completely similar).

