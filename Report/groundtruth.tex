\subsection{Ground Truth}
\label{sec:groundtruth}

James Kobielus \cite{kobielus}, from IBM Big Data and Analytics Hub, describes ground truth as ``a golden standard to which the learning algorithm needs to adapt''. In most cases, a training dataset labeled by human experts is needed to provide data patterns for the learning algorithm to use as baseline. This type of machine learning is referred to as supervised learning. The other two main approaches, unsupervised learning and reinforcement learning, aim to automate this process from the data itself, not labeled by humans.

In our case we apply a supervised learning model using a ground truth dataset constructed primarily from the clickstream data query logs. We use a subset of 1000 articles having the most outgoing clicks. We refer to these articles as being the \textit{prominent} articles. Each of these prominent articles contain links to other articles which add to a total of 144k uniquely referenced articles and 283k article $(p,\beta)$ pairs where $p$ is a prominent article and $\beta$ is a \textit{prominent-linked} article.

%feature table has to be here so that it can be displayed on the next page
\input{featuretable.tex}

The clickstream data does not contain pairs where the number of referrals from $p$ to $\beta$ is below 10 and therefore we supplement our ground truth data using the Wikipedia article dumps for the same period of time as the recording of clickstream data. In this way, we can extract article pairs that do not appear in the clickstream data because of the 10-click limit (set to protect privacy) -- this adds a substantial amount of ``unpopular'' article links to our set so that we avoid a bias towards more clicked links.