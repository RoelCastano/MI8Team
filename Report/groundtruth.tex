\subsection{Ground Truth}

James Kobielus \cite{kobielus}, from IBM Big Data and Analytics Hub, describes ground truth as ``a golden standard to which the learning algorithm needs to adapt''. In most cases, a training data set labeled by human experts is needed to provide data patterns for the learning algorithm to use as baseline. This type of machine learning is referred to as supervised learning. The other two main approaches, unsupervised learning and reinforcement learning, attempt to automate the distillation of knowledge from data not previously labeled by human experts. \\

In our case to apply supervised learning, we built the ground truth for our model primarily using the Clickstream data. We use a subset of articles consisting of the 1000 articles having the most outgoing clicks. We refer to these articles as being \textit{prominent articles}. Each of these prominent articles contain links to other articles so that our ground truth dataset contains 186k uniquely referenced articles and 343k article \fxnote{Heyo} $(A,B)$ pairs where $A$ is a prominent article and $B$ is a prominent-linked article. \\

The clickstream data does not contain pairs were the navigational traffic is below 10 referrals and therefore we use the Wikipedia article dump for the same period of time where we can extract links that does not appear in the clickstream data. A large fraction of existing links are having less than 10 clicks. \\

INCOMPLETE