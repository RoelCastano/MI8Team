\subsection{Ground Truth}

James Kobielus \cite{kobielus}, from IBM Big Data and Analytics Hub, describes ground truth as ``a golden standard to which the learning algorithm needs to adapt''. In most cases, a training data set labeled by human experts is needed to provide data patterns for the learning algorithm to use as baseline. This type of machine learning is referred to as supervised learning. The other two main approaches, unsupervised learning and reinforcement learning, attempt to automate the distillation of knowledge from data not previously labeled by human experts. \\

In our case we apply a supervised learning model using a ground truth dataset constructed primarily from the clickstream data query logs. We use a subset of 1000 articles having the most outgoing clicks. We refer to these articles as being \textit{prominent} articles. Each of these prominent articles contain links to other articles so that we in total have \fxnote{Insert correct numbers} 186k uniquely referenced articles and 343k article $(\alpha,\beta)$ pairs where $\alpha$ is a prominent article and $\beta$ is a \textit{prominent-linked} article. \\

The clickstream data does not contain pairs where the the number of referrals from $\alpha$ to $\beta$ is below 10 and therefore we supplement our ground truth data using the Wikipedia article dumps for the same period of time as the recording of clickstream data. In this way we can extract article pairs that does not appear in the clickstream data because of the 10-click limit (set to protect privacy) -- this adds a substantial amount of ``unpopular'' article links to our set so that we avoid a bias towards more clicked links. \fxnote{INCOMPLETE} \\